# Hunger Logs
Why are we incrementally hashing with the update to the SHA256 structure?
### 1. Consider hashing data coming in a stream, keeping context or "state" of the hashing context
will allow us to incrementally hash chunks of data, which can be more memory efficient.
### 2. For security purposes, auditing is a big part of the blockchain as we should be able
to see tampering in the history since each hashing action creates a unique identifier (block by block, transaction by transaction,etc)
### 3. The data is often times not available to process all at once, so once again hashing streaming data in more manageable chunks is an advantage.


The for loop for converting the binary hash into its hexadecimal representation is written in the hash computation in block.cpp.
Of note is that the setw(2) ensures the stream happens in pairs whereas the setfill('0') makes sure there aren't any unpaired char
in the final stream and will fill 'pad' the pair with a 0;


Learned about the Merkle root.
1. A unique identifier for an entire set of transactions that starts with:
    a. hashing each transaction with a SHA-256 cyrptographic hash function
    b. converting to hashed pairs
    c. repeating the pairwise hashing until a single hash is created, the Merkle root (this allows for 'tamper-evident' and concise representation of the data set)
2. No need for nodes to process individual transactions, all we need to do now is to check the merkle root attached to the header to efficiently verify transaction
    including in the block


